{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9175a8e",
   "metadata": {},
   "source": [
    "### Setting up Directory for Flee Scenario \n",
    "\n",
    "Start by creating a directory with the name of the desired country followed by a date (e.g., nigeria2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26182e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the country name\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(country):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(country)\n",
    "    print(f\"Directory '{country}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{country}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141327f5",
   "metadata": {},
   "source": [
    "### Create Locations File\n",
    "\n",
    "This builder extracts cities/towns populations from an HTML file and stores it in the population.csv file, however before that there are requirements as follow:\n",
    "\n",
    "1. Generate and add `acled.csv` in the created directory (e.g., nigeria2016).\n",
    "\n",
    "For more information, visit https://flee.readthedocs.io/en/master/Simulation_instance_construction/\n",
    "\n",
    "2. Generate and add `popolation.html` for the desired country in the same created directory.\n",
    "\n",
    "The country can be found by searching the https://www.citypopulation.de/ website (e.g., https://www.citypopulation.de/en/nigeria/cities/).\n",
    "\n",
    "Once the webpage with population tables is found, store the webpage \"CTRL+S\" as an HTML file (e.g., population.html) and place in in the same created directory.\n",
    "\n",
    "The builder accesses the tables in HTML file and extract data from its tables (e.g., table[0], table[1]). Specify a table in the code, line 19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc4cbc",
   "metadata": {},
   "source": [
    "### Extracts cities/towns populations from an HTML file \n",
    "\n",
    "The builder accesses the tables in HTML file and extract data from a specified table (e.g., table[0], table[1]). Specify the table in the code \"line 19\".\n",
    "\n",
    "The desired country can be found by searching the https://www.citypopulation.de/ site (e.g., https://www.citypopulation.de/en/nigeria/cities/).\n",
    "\n",
    "After locating the webpage containing the data, store the webpage \"CTRL+S\" as an HTML file (e.g., population.html) and place in in the created directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# This also becomes the name of the directory where files are stored.\n",
    "country = 'nigeria2016'\n",
    "\n",
    "html_file = '{}/population.html'.format(country, country)\n",
    "\n",
    "if os.path.isfile(html_file):\n",
    "    tables = pd.read_html(html_file)\n",
    "    # Continue with further processing of the tables\n",
    "else:\n",
    "    print(\"The file '{}' is not found.\".format(html_file))\n",
    "\n",
    "# Uncomment to make sure the tables are accessed from the html file\n",
    "# print(tables)\n",
    "\n",
    "# Specify table with major cities' names and population\n",
    "table = tables[0]\n",
    "\n",
    "# Uncomment to make sure the desired table is accessed from tables\n",
    "# print(table)\n",
    "\n",
    "# Please make sure the column names exist in the specified table\n",
    "selected_columns = table[['Name', 'Population Census (Cf) 2006-03-21']]\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "selected_columns = selected_columns.dropna()\n",
    "\n",
    "# Rename columns\n",
    "selected_columns.columns = ['name', 'population']\n",
    "\n",
    "# Filter rows with population greater than 10,000\n",
    "selected_columns = selected_columns[selected_columns['population'] > 10000]\n",
    "\n",
    "# Save the data to a CSV file\n",
    "output_file = '{}/population.csv'.format(country)\n",
    "selected_columns.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'{country}/population.csv created. Please inspect the file for non-standard characters!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c97f02",
   "metadata": {},
   "source": [
    "### Create Locations File\n",
    "\n",
    "This builder extracts locations using acled.csv and created population.csv, and stores them in locations.csv file.\n",
    "\n",
    "Adjust the conflict_threshold to only add conflict zones with conflict periods greater than the threshold to location types. This will seperate towns or non-conflict zones from the conflict zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48110d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import calendar as cal\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import wikipedia\n",
    "import wbdata\n",
    "\n",
    "def date_format(in_date):\n",
    "    # converting date from textbased to dd-mm-yyyy format\n",
    "    if \"-\" in in_date:\n",
    "        split_date = in_date.split(\"-\")\n",
    "    else:\n",
    "        split_date = in_date.split(\" \")\n",
    "\n",
    "    month_num = month_convert(split_date[1])\n",
    "    if int(split_date[2]) < 50:\n",
    "        year = int(split_date[2]) + 2000\n",
    "    else:\n",
    "        year = int(split_date[2])\n",
    "    out_date = split_date[0] + \"-\" + str(month_num) + \"-\" + str(year)\n",
    "    return out_date\n",
    "\n",
    "def month_convert(month_name):\n",
    "    months = {\n",
    "    \"jan\": \"01\", \"january\": \"01\",\n",
    "    \"feb\": \"02\", \"february\": \"02\",\n",
    "    \"mar\": \"03\", \"march\": \"03\",\n",
    "    \"apr\": \"04\", \"april\": \"04\",\n",
    "    \"may\": \"05\", \"may\": \"05\",\n",
    "    \"jun\": \"06\", \"june\": \"06\",\n",
    "    \"jul\": \"07\", \"july\": \"07\",\n",
    "    \"aug\": \"08\", \"august\": \"08\",\n",
    "    \"sep\": \"09\", \"september\": \"09\",\n",
    "    \"oct\": \"10\", \"october\": \"10\",\n",
    "    \"nov\": \"11\", \"november\": \"11\",\n",
    "    \"dec\": \"12\", \"december\": \"12\"\n",
    "    }\n",
    "\n",
    "    # Convert the month name to lowercase and strip leading/trailing whitespace\n",
    "    month_name = month_name.strip().lower()\n",
    "\n",
    "    # Look up the month number in the dictionary\n",
    "    if month_name in months:\n",
    "        month_num = months[month_name]\n",
    "        #print(f\"The month number for {month_name} is {month_num}.\")\n",
    "    else:\n",
    "        print(\"Invalid month name entered.\")\n",
    "\n",
    "    return month_num\n",
    "\n",
    "def between_date(d1, d2):\n",
    "    # Gets difference between two dates in string format \"dd-mm-yyyy\"\n",
    "    d1list = d1.split(\"-\")\n",
    "    d2list = d2.split(\"-\")\n",
    "    date1 = datetime(int(d1list[2]), int(d1list[1]), int(d1list[0]))\n",
    "    date2 = datetime(int(d2list[2]), int(d2list[1]), int(d2list[0]))\n",
    "\n",
    "    return abs((date1 - date2).days)  # Maybe add +1\n",
    "\n",
    "def drop_rows(inputdata, columnname, dropparameter):\n",
    "    removedrows = inputdata.index[\n",
    "        inputdata[columnname] <= dropparameter].tolist()\n",
    "    outputdata = inputdata.drop(removedrows)\n",
    "    return outputdata\n",
    "\n",
    "def get_state_population(state_name, population_input_file):\n",
    "    df = pd.read_csv(population_input_file)\n",
    "    filtered_df = df[df['name'] == state_name]\n",
    "    if len(filtered_df) > 0:\n",
    "        population = filtered_df['population'].values[0]\n",
    "        return population\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_city_population(city_name,population_input_file):\n",
    "    country_code = 'ML'\n",
    "    url = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities/{0}\".format(get_wikidata_id(city_name))\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"6e1b07b54fmsh14df87e58d9db7bp175272jsn85fd0398365f\",\n",
    "        \"X-RapidAPI-Host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "    }\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.status_code == 404:\n",
    "        get_state_population(city_name,population_input_file)\n",
    "\n",
    "    else:\n",
    "        data = response.json()\n",
    "        population = data[\"data\"]['population']\n",
    "        return population\n",
    "    \n",
    "def filter_table(df, colname, adminlevel):\n",
    "    if adminlevel == \"admin1\":\n",
    "        adminlist = df.admin1.unique()\n",
    "    elif adminlevel == \"location\":\n",
    "        adminlist = df.location.unique()\n",
    "    else:\n",
    "        adminlist = df.admin2.unique()\n",
    "\n",
    "    newdf = pd.DataFrame()  \n",
    "\n",
    "    for admin in adminlist:\n",
    "        tempdf = df.loc[df[adminlevel] == admin]\n",
    "        tempdf.sort_values(colname, ascending=True)\n",
    "        newdf = pd.concat([newdf, tempdf.tail(1)])\n",
    "\n",
    "    return newdf\n",
    "\n",
    "\n",
    "def acled2locations(country, start_date, filter_opt, location_type=\"admin1\", conflict_threshold=100):\n",
    "    current_dir = os.getcwd()\n",
    "    input_file = os.path.join(current_dir, country, \"acled.csv\") \n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except:\n",
    "        print(\"Runtime Error: File Cannot be found\")\n",
    "        return\n",
    "    \n",
    "    df = df[[\"event_date\", \"country\", \"admin1\", \"admin2\", \"location\", \"latitude\", \"longitude\", \"event_type\", \"sub_event_type\", \"fatalities\"]]\n",
    "    \n",
    "    event_dates = df[\"event_date\"].tolist()\n",
    "    \n",
    "    formatted_event_dates = [date_format(date) for date in event_dates]\n",
    "    \n",
    "    # Calculate the number of days between each event's date and the start_date\n",
    "    conflict_dates = [between_date(d, start_date) for d in formatted_event_dates]\n",
    "    \n",
    "    # Add days to the df\n",
    "    df['conflict_date'] = conflict_dates\n",
    "    \n",
    "    # Including all events\n",
    "    fatalities_threshold = 0\n",
    "    \n",
    "    # Dropping rows based on the 'fatalities_threshold'    \n",
    "    df = drop_rows(df, 'fatalities', fatalities_threshold)\n",
    "    \n",
    "    # Sorting the df by 'conflict_date' and 'admin1' and drops duplicate rows\n",
    "    df = df.sort_values([\"conflict_date\", \"admin1\"]).drop_duplicates([\"conflict_date\", \"admin1\"])\n",
    "    \n",
    "    if filter_opt == 'earliest':\n",
    "        filter_opt = 'conflict_date'\n",
    "\n",
    "    try:\n",
    "        df = filter_table(df, filter_opt, location_type)\n",
    "    except:\n",
    "        print(\"Runtime error: filter_opt value must be earliest or fatalities\")\n",
    "        \n",
    "    output_df = df[['admin1', 'country', 'latitude', 'longitude', 'conflict_date']]\n",
    "    \n",
    "    output_df.columns = ['name', 'country', 'latitude', 'longitude', 'conflict_date'] \n",
    "    \n",
    "    # Create two DataFrames for towns and conflict zones\n",
    "    towns_df = df[df['conflict_date'] <= conflict_threshold].copy()\n",
    "    conflict_zones_df = df[df['conflict_date'] > conflict_threshold].copy()\n",
    "    \n",
    "    # Assign location types\n",
    "    towns_df['location_type'] = 'town'\n",
    "    conflict_zones_df['location_type'] = 'conflict_zone'\n",
    "    \n",
    "    # Merge the DataFrames back together if necessary\n",
    "    merged_df = pd.concat([towns_df, conflict_zones_df])\n",
    "    \n",
    "    # Create a dictionary to store location names from population.csv as keys and their populations as values\n",
    "    population_dict = {row['name']: row['population'] for index, row in population_df.iterrows()}\n",
    "    \n",
    "    # Add population to the merged_df based on 'admin1' names\n",
    "    merged_df['population'] = [population_dict.get(name, 0) for name in merged_df['admin1']]\n",
    "    \n",
    "    # Replace any infinite or NaN values with 0 and convert population to integers\n",
    "    merged_df['population'] = merged_df['population'].replace([np.inf, -np.inf, np.nan], 0)\n",
    "    merged_df['population'] = merged_df['population'].astype(int)\n",
    "    \n",
    "    # Reorder columns and rename them\n",
    "    merged_df = merged_df[['admin1', 'country', 'latitude', 'longitude', 'location_type', 'conflict_date', 'population']]\n",
    "    merged_df.columns = ['name', 'country', 'latitude', 'longitude', 'location_type', 'conflict_date', 'population']\n",
    "\n",
    "    # Save the final 'locations.csv' file\n",
    "    output_file = os.path.join(country, \"locations.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f'{country}/locations.csv created. Please inspect the file for unwanted anomalies!')\n",
    "\n",
    "    \n",
    "# Please replace the country name \n",
    "country = \"nigeria2016\"\n",
    "start_date = \"01-01-2016\"\n",
    "filter_opt = 'earliest'\n",
    "location_type = \"admin1\"\n",
    "conflict_threshold = 100\n",
    "\n",
    "population_input_file = os.path.join(country, \"population.csv\")\n",
    "\n",
    "acled2locations(country, start_date, filter_opt, location_type=\"admin1\", conflict_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2238a5c",
   "metadata": {},
   "source": [
    "### Create Conflicts File\n",
    "\n",
    "This builder constructs conflicts.csv from created locations.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Replace path with the actual directory path\n",
    "flee_path = '/home/mghorbani/workspace/flee'\n",
    "sys.path.append(flee_path)\n",
    "\n",
    "# Now you can import the modules from the 'flee' package\n",
    "from flee.InputGeography import InputGeography\n",
    "\n",
    "def find_column_index(header, column_name):\n",
    "    # Find the index of a column by matching its name in the header\n",
    "    for i, col in enumerate(header):\n",
    "        if col == column_name:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def location2conflict(simulation_period, input_file, output_file):\n",
    "    ig = InputGeography()\n",
    "    ig.ReadLocationsFromCSV(input_file)\n",
    "\n",
    "    with open(input_file, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        conflict_zone_index = find_column_index(header, \"location_type\")\n",
    "        conflict_date_index = find_column_index(header, \"conflict_date\")\n",
    "\n",
    "    # Create the header string for the output file without the \"name\" column\n",
    "    output_header_string = \"day\"\n",
    "    for l in ig.locations:\n",
    "        if l[conflict_zone_index] == \"conflict_zone\":\n",
    "            output_header_string += \",%s\" % l[0]\n",
    "    output_header_string += \"\\n\"\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(output_header_string)\n",
    "\n",
    "        for t in range(0, simulation_period):\n",
    "            output = \"%s\" % t\n",
    "            for l in ig.locations:\n",
    "                if l[conflict_zone_index] == \"conflict_zone\":\n",
    "                    confl_date = int(l[conflict_date_index])\n",
    "                    if confl_date <= t:\n",
    "                        output += \",1\"\n",
    "                    else:\n",
    "                        output += \",0\"\n",
    "            output += \"\\n\"\n",
    "            file.write(output)\n",
    "\n",
    "\n",
    "# Set the values for simulation_period, input_file, and output_file\n",
    "simulation_period = 365\n",
    "\n",
    "# Please specify a country name\n",
    "country = \"nigeria2016\"\n",
    "\n",
    "input_file = os.path.join(country, \"locations.csv\")\n",
    "\n",
    "output_file = os.path.join(country, \"conflicts.csv\")\n",
    "\n",
    "# Call the function location2conflict\n",
    "location2conflict(simulation_period, input_file, output_file)\n",
    "\n",
    "print(f'{country}/conflicts.csv created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bcfe1",
   "metadata": {},
   "source": [
    "###  Create Conflict Scenario\n",
    "\n",
    "This builder simulates a conflict scenario in a given country using custom distribution function. \n",
    "\n",
    "Change function parameters to create different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a365ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "# Read \n",
    "def read_conflict_zones(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        conflict_zones = df.columns[1:].tolist()\n",
    "        conflict_zones = [zone for zone in conflict_zones if zone]  # Exclude empty headers\n",
    "        return conflict_zones\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found: \", filename)\n",
    "        return []\n",
    "    \n",
    "    \n",
    "# Custom function to generate all zeros csv file\n",
    "def generate_conflict_zones_csv(filename, conflict_zones, period):\n",
    "    data = {'Days': list(range(period))}\n",
    "    data.update({zone: [0] * period for zone in conflict_zones})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "\n",
    "# Custom distribution function\n",
    "def custom_distribution(intensity, x):\n",
    "    max_value = intensity\n",
    "    peak_day = 365\n",
    "    std_deviation = 800\n",
    "    variation_factor = 0.01  # Adjust the variation factor as desired\n",
    "    \n",
    "    spreading_factor = np.exp(-((x - peak_day) / std_deviation) ** 2)\n",
    "    \n",
    "    # Add random fluctuations to the spreading factor\n",
    "    spreading_factor += np.random.normal(0, variation_factor, len(x))\n",
    "    \n",
    "    y = max_value * spreading_factor\n",
    "    return y\n",
    "\n",
    "# Specify the simulation country\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Create the path to input file\n",
    "input_file = os.path.join(country, 'conflicts.csv')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Call the function to extract the conflict_zones from the file\n",
    "conflict_zones = read_conflict_zones(input_file)\n",
    "\n",
    "# Set the intensity as maximum number of conflict zones \n",
    "intensity = len(conflict_zones)\n",
    "\n",
    "# Generate x-axis values\n",
    "start_date = date(2016, 1, 1)\n",
    "current_date = date(2016, 12, 31)\n",
    "days_passed = (current_date - start_date).days\n",
    "\n",
    "# Specify the simulation period for forcasting\n",
    "period = days_passed * 2\n",
    "\n",
    "# Generate x-axis values using a custom_distributions\n",
    "x1 = np.linspace(0, days_passed, num=days_passed).astype(int)\n",
    "x2 = np.linspace(days_passed, period, num=period - days_passed).astype(int)\n",
    "\n",
    "# Generate y-axis values using conflicts values\n",
    "y1 = df.iloc[:, 1:days_passed + 1].sum(axis=1)[:days_passed]\n",
    "\n",
    "# Generate y-axis values using a custom_distribution\n",
    "y2 = custom_distribution(intensity, x2)\n",
    "\n",
    "# Combine the generated data\n",
    "x = np.concatenate((x1, x2))\n",
    "y = np.concatenate((y1, y2))\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# Plot graph\n",
    "ax1.plot(x, y)\n",
    "ax1.set_title('Custom Distribution')\n",
    "ax1.set_xlabel('Days')\n",
    "ax1.set_ylabel('Conflict Zones')\n",
    "\n",
    "# Convert y values to integers\n",
    "y = [int(val) for val in y]\n",
    "\n",
    "# Create path to modified CSV file\n",
    "output_file = os.path.join(country, \"simulated-conflicts.csv\")\n",
    "\n",
    "# Call the function to generate all zeros csv file\n",
    "generate_conflict_zones_csv(output_file, conflict_zones, period)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_modified = pd.read_csv(output_file)\n",
    "\n",
    "# Update the modified DataFrame with the new y values\n",
    "modified_rows = []\n",
    "y_index = 0\n",
    "for _, row in df_modified.iterrows():\n",
    "    row = row.values\n",
    "    if y_index >= len(y):\n",
    "        break\n",
    "\n",
    "    number = y[y_index]\n",
    "    y_index += 1\n",
    "    assigned_count = 0\n",
    "    for i in range(1, len(row)):\n",
    "        if assigned_count < number:\n",
    "            row[i] = 1\n",
    "            assigned_count += 1\n",
    "        else:\n",
    "            row[i] = 0\n",
    "\n",
    "    modified_rows.append(row)\n",
    "    \n",
    "# Create modified DataFrame\n",
    "modified_df = pd.DataFrame(modified_rows, columns=df.columns)\n",
    "\n",
    "# Compute the sum of each row (excluding the '#Day' column)\n",
    "sum_values = modified_df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Plot the summed values\n",
    "ax2.plot(x, sum_values)\n",
    "ax2.set_title('simulated-conflicts.csv')\n",
    "ax2.set_xlabel('Days')\n",
    "\n",
    "# Save the modified DataFrame to the output file\n",
    "modified_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'{country}/simulated-conflicts.csv created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e02fce",
   "metadata": {},
   "source": [
    "### Create Routes Between Locations\n",
    "\n",
    "This builder construct routes.csv from created locations.csv file.\n",
    "\n",
    "Before creating routes, add camps to the locations.csv, following the header format. \n",
    "\n",
    "Example: There is a sample file \"camps.csv\" in nigeria2016 directory. Copy its content to the end of locations.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import csv\n",
    "import webbrowser\n",
    "\n",
    "# Please replace country if required\n",
    "country = \"nigeria2016\"\n",
    "\n",
    "# Read the locations from the CSV file\n",
    "df = pd.read_csv(f'{country}/locations.csv')\n",
    "\n",
    "# Initialise a list to store route information\n",
    "routes = []\n",
    "\n",
    "# Calculate routes and distances between locations\n",
    "for i, loc1 in df.iterrows():\n",
    "    for j, loc2 in df.iterrows():\n",
    "        if i != j:\n",
    "            loc1_coords = (loc1['latitude'], loc1['longitude'])\n",
    "            loc2_coords = (loc2['latitude'], loc2['longitude'])\n",
    "            \n",
    "            # Calculate the distance between loc1_coords and loc2_coords\n",
    "            distance = geodesic(loc1_coords, loc2_coords).km\n",
    "            \n",
    "            # Add route information to the list\n",
    "            routes.append([loc1['name'], loc2['name'], distance, 0]) \n",
    "\n",
    "# Save the routes to a CSV file\n",
    "with open(f'{country}/routes.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['location1', 'location2', 'distance', 'foreced_redirection']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the route information\n",
    "    for route in routes:\n",
    "        writer.writerow({'name1': route[0], 'name2': route[1], 'distance': round(route[2], 3), 'foreced_redirection': route[3]})\n",
    "\n",
    "# Add the map layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "print(f'{country}/routes.csv created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef01c5f",
   "metadata": {},
   "source": [
    "### Experimental Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e7ca3",
   "metadata": {},
   "source": [
    "### Create More Detailed Map and Coordinates\n",
    "\n",
    "This builder construct locations and routes using Google Map API, and stores routes coordinates. \n",
    "\n",
    "Please generate your API_KEY and add it to the code, line 85."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db798644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "import folium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import polyline\n",
    "import csv\n",
    "\n",
    "# Function to decode a polyline string into a list of locations\n",
    "def decode_polyline(polyline_str):\n",
    "    return polyline.decode(polyline_str)\n",
    "\n",
    "# Please replace country if required\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Read the locations from the CSV file\n",
    "df = pd.read_csv('{}/locations.csv'.format(country))\n",
    "\n",
    "# Create layers for location markers and routes\n",
    "layer1 = folium.FeatureGroup(name='Locations')\n",
    "layer2 = folium.FeatureGroup(name='Routes')\n",
    "\n",
    "# Initialize the map\n",
    "map_center = (df['latitude'].mean(), df['longitude'].mean())\n",
    "m = folium.Map(location=map_center, zoom_start=6)\n",
    "\n",
    "# Create markers for conflict zones, towns, and camps within the distance threshold\n",
    "conflict_zones = []\n",
    "towns = []\n",
    "camps = []\n",
    "distance_threshold = 1000  # Set the distance threshold in kilometers\n",
    "locations_not_found = []  # List to store locations not found within the threshold\n",
    "for index, row in df.iterrows():\n",
    "    location = [row['latitude'], row['longitude']]\n",
    "    if row['location_type'] == 'conflict_zone':\n",
    "        conflict_zones.append({'location': location, 'name': row['name']})\n",
    "        if any(geodesic(location, cz['location']).km <= distance_threshold for cz in conflict_zones):\n",
    "            layer1.add_child(folium.CircleMarker(location=tuple(location), radius=6, color='red', fill=True, fill_color='red',\n",
    "                                                 popup=row['name']))\n",
    "        else:\n",
    "            locations_not_found.append(row['name'])\n",
    "    elif row['location_type'] == 'town':\n",
    "        towns.append({'location': location, 'name': row['name']})\n",
    "        if any(geodesic(location, t['location']).km <= distance_threshold for t in towns):\n",
    "            layer1.add_child(folium.CircleMarker(location=tuple(location), radius=6, color='orange', fill=True, fill_color='orange',\n",
    "                                                 popup=row['name']))\n",
    "        else:\n",
    "            locations_not_found.append(row['name'])\n",
    "    elif row['location_type'] == 'camp':\n",
    "        camps.append({'location': location, 'name': row['name']})\n",
    "        if any(geodesic(location, c['location']).km <= distance_threshold for c in camps):\n",
    "            layer1.add_child(folium.CircleMarker(location=tuple(location), radius=6, color='green', fill=True, fill_color='green',\n",
    "                                                 popup=row['name']))\n",
    "        else:\n",
    "            locations_not_found.append(row['name'])\n",
    "\n",
    "# Create markers for locations not found within the threshold\n",
    "for location_name in locations_not_found:\n",
    "    print(f\"Location not found within the threshold: {location_name}\")\n",
    "    \n",
    "routes = []\n",
    "# Connect all locations using Google Maps Directions API\n",
    "for loc1 in df.itertuples():\n",
    "    loc1_type = loc1.location_type\n",
    "    loc1_coords = (loc1.latitude, loc1.longitude)\n",
    "\n",
    "    for loc2 in df.itertuples():\n",
    "        loc2_type = loc2.location_type\n",
    "        loc2_coords = (loc2.latitude, loc2.longitude)\n",
    "\n",
    "        if loc1 != loc2:\n",
    "            # Calculate the distance between loc1_coords and loc2_coords\n",
    "            distance = geodesic(loc1_coords, loc2_coords).km\n",
    "\n",
    "            # Limit the search to locations within a certain distance threshold\n",
    "            if distance <= 1000:  # Adjust the distance threshold as needed\n",
    "                # Use Google Maps Directions API to obtain the route\n",
    "                api_url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
    "                params = {\n",
    "                    \"origin\": f\"{loc1_coords[0]},{loc1_coords[1]}\",\n",
    "                    \"destination\": f\"{loc2_coords[0]},{loc2_coords[1]}\",\n",
    "                    \"key\": \"AIzaSyCFayFsbHfcA0GuOfhqaRqec3w90A9lbt0\" # Replace with your own Google Maps API key\n",
    "                }\n",
    "                response = requests.get(api_url, params=params)\n",
    "                data = response.json()\n",
    "\n",
    "                # Extract the polyline representing the route\n",
    "                if data[\"status\"] == \"OK\":\n",
    "                    polyline_points = data[\"routes\"][0][\"overview_polyline\"][\"points\"]\n",
    "                    polyline_locations = decode_polyline(polyline_points)\n",
    "\n",
    "                    # Create a route with the polyline locations\n",
    "                    if loc1_type == 'town' or loc2_type == 'town':\n",
    "                        color = 'purple'  # Set color for routes involving towns\n",
    "                    elif loc1_type != loc2_type:\n",
    "                        color = 'green'  # Set color for routes between different location types\n",
    "                    else:\n",
    "                        color = 'blue'  # Set color for routes between the same location types\n",
    "                    layer2.add_child(folium.PolyLine(locations=polyline_locations, color=color, weight=2.5))\n",
    "\n",
    "                    # Append the route information to the routes list\n",
    "                    forced_redirection = int(loc1_type != loc2_type and loc1_type != 'town' and loc2_type != 'town')\n",
    "                    routes.append([loc1.name, loc2.name, round(distance, 3), forced_redirection])\n",
    "                else:\n",
    "                    print(f\"No route found between {loc1.name} and {loc2.name}\")\n",
    "\n",
    "\n",
    "# Save the routes to a CSV file\n",
    "output_file = '{}/routes.csv'.format(country)\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"name1\", \"name2\", \"distance\", \"forced_redirection\"])\n",
    "    writer.writerows(routes)\n",
    "\n",
    "# Add the layers to the map\n",
    "m.add_child(layer2)\n",
    "m.add_child(layer1)\n",
    "\n",
    "# Add the layer control to the map\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Save map in html\n",
    "m.save('{}/map.html'.format(country))\n",
    "\n",
    "# # If any location is not shown, please add it to the locations.csv\n",
    "webbrowser.open('{}/map.html'.format(country))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4f138",
   "metadata": {},
   "source": [
    "### Store Route Coordinates in JSON Format\n",
    "\n",
    "This builder constructs routes and stores them in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b09125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigeria2016/routes.json created.\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "import folium\n",
    "import requests\n",
    "import pandas as pd\n",
    "from geopy.distance import geodesic\n",
    "import polyline\n",
    "import csv\n",
    "\n",
    "# Function to decode a polyline string into a list of locations\n",
    "def decode_polyline(polyline_str):\n",
    "    return polyline.decode(polyline_str)\n",
    "\n",
    "# Please replace country if required\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Read the locations from the CSV file\n",
    "df = pd.read_csv('{}/locations.csv'.format(country))\n",
    "\n",
    "# Create markers for locations not found within the threshold\n",
    "locations_not_found = []  # List to store locations not found within the threshold\n",
    "\n",
    "# Create a dictionary to store routes\n",
    "routes = []\n",
    "\n",
    "# Connect all locations using Google Maps Directions API\n",
    "for loc1 in df.itertuples():\n",
    "    loc1_coords = (loc1.latitude, loc1.longitude)\n",
    "\n",
    "    for loc2 in df.itertuples():\n",
    "        loc2_coords = (loc2.latitude, loc2.longitude)\n",
    "\n",
    "        if loc1 != loc2:\n",
    "            # Calculate the distance between loc1_coords and loc2_coords\n",
    "            distance = geodesic(loc1_coords, loc2_coords).km\n",
    "\n",
    "            # Limit the search to locations within a certain distance threshold\n",
    "            if distance <= 1000:  # Adjust the distance threshold as needed\n",
    "                # Use Google Maps Directions API to obtain the route\n",
    "                api_url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
    "                params = {\n",
    "                    \"origin\": f\"{loc1_coords[0]},{loc1_coords[1]}\",\n",
    "                    \"destination\": f\"{loc2_coords[0]},{loc2_coords[1]}\",\n",
    "                    \"key\": \"AIzaSyCFayFsbHfcA0GuOfhqaRqec3w90A9lbt0\"  # Replace with your own Google Maps API key\n",
    "                }\n",
    "                response = requests.get(api_url, params=params)\n",
    "                data = response.json()\n",
    "\n",
    "                # Extract the polyline representing the route\n",
    "                if data[\"status\"] == \"OK\":\n",
    "                    polyline_points = data[\"routes\"][0][\"overview_polyline\"][\"points\"]\n",
    "                    polyline_locations = decode_polyline(polyline_points)\n",
    "\n",
    "                    # Create a route with the polyline locations\n",
    "                    route = {\n",
    "                        \"name1\": loc1.name, \"name2\": loc2.name, \"route\": polyline_locations\n",
    "                    }\n",
    "                    routes.append(route)\n",
    "                else:\n",
    "                    locations_not_found.append((loc1.name, loc2.name))\n",
    "\n",
    "# Print locations not found\n",
    "for location_names in locations_not_found:\n",
    "    print(f\"Route not found between {location_names[0]} and {location_names[1]}\")\n",
    "\n",
    "# Save routes as a dictionary to a JSON file\n",
    "import json\n",
    "with open('{}/routes.json'.format(country), 'w') as json_file:\n",
    "    json.dump(routes, json_file)\n",
    "    \n",
    "print(f'{country}/routes.json created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bafbfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
