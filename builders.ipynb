{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9175a8e",
   "metadata": {},
   "source": [
    "### Setting up Directory for Flee Scenario \n",
    "\n",
    "Start by creating a directory with the name of the desired country followed by a date (e.g., nigeria2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26182e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the country name\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Check if the directory already exists\n",
    "if not os.path.exists(country):\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(country)\n",
    "    print(f\"Directory '{country}' created.\")\n",
    "else:\n",
    "    print(f\"Directory '{country}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141327f5",
   "metadata": {},
   "source": [
    "### Create Locations File\n",
    "\n",
    "This builder extracts cities/towns populations from an HTML file and stores it in the population.csv file, however before that there are requirements as follow:\n",
    "\n",
    "1. Generate and add `acled.csv` in the created directory (e.g., nigeria2016).\n",
    "\n",
    "For more information, visit https://flee.readthedocs.io/en/master/Simulation_instance_construction/\n",
    "\n",
    "2. Generate and add `popolation.html` for the desired country in the same created directory.\n",
    "\n",
    "The country can be found by searching the https://www.citypopulation.de/ website (e.g., https://www.citypopulation.de/en/nigeria/cities/).\n",
    "\n",
    "Once the webpage with population tables is found, store the webpage \"CTRL+S\" as an HTML file (e.g., population.html) and place in in the same created directory.\n",
    "\n",
    "The builder accesses the tables in HTML file and extract data from its tables (e.g., table[0], table[1]). Specify a table in the code, line 19."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc4cbc",
   "metadata": {},
   "source": [
    "### Extracts cities/towns populations from an HTML file \n",
    "\n",
    "The builder accesses the tables in HTML file and extract data from a specified table (e.g., table[0], table[1]). Specify the table in the code \"line 19\".\n",
    "\n",
    "The desired country can be found by searching the https://www.citypopulation.de/ site (e.g., https://www.citypopulation.de/en/nigeria/cities/).\n",
    "\n",
    "After locating the webpage containing the data, store the webpage \"CTRL+S\" as an HTML file (e.g., population.html) and place in in the created directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676f72f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# This also becomes the name of the directory where files are stored.\n",
    "country = 'nigeria2016'\n",
    "\n",
    "html_file = '{}/population.html'.format(country, country)\n",
    "\n",
    "if os.path.isfile(html_file):\n",
    "    tables = pd.read_html(html_file)\n",
    "    # Continue with further processing of the tables\n",
    "else:\n",
    "    print(\"The file '{}' is not found.\".format(html_file))\n",
    "\n",
    "# Uncomment to make sure the tables are accessed from the html file\n",
    "# print(tables)\n",
    "\n",
    "# Specify table with major cities' names and population\n",
    "table = tables[0]\n",
    "\n",
    "# Uncomment to make sure the desired table is accessed from tables\n",
    "# print(table)\n",
    "\n",
    "# Please make sure the column names exist in the specified table\n",
    "selected_columns = table[['Name', 'Population Census (Cf) 2006-03-21']]\n",
    "\n",
    "# Drop rows with missing values in the selected columns\n",
    "selected_columns = selected_columns.dropna()\n",
    "\n",
    "# Rename columns\n",
    "selected_columns.columns = ['name', 'population']\n",
    "\n",
    "# Filter rows with population greater than 10,000\n",
    "selected_columns = selected_columns[selected_columns['population'] > 10000]\n",
    "\n",
    "# Save the data to a CSV file\n",
    "output_file = '{}/population.csv'.format(country)\n",
    "selected_columns.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'{country}/population.csv created. Please inspect the file for non-standard characters!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c97f02",
   "metadata": {},
   "source": [
    "### Create Locations File\n",
    "\n",
    "This builder extracts locations using acled.csv and created population.csv, and stores them in locations.csv file.\n",
    "\n",
    "Adjust the conflict_threshold to only add conflict zones with conflict periods greater than the threshold to location types. This will seperate towns or non-conflict zones from the conflict zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48110d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "import calendar as cal\n",
    "from datetime import datetime\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import wikipedia\n",
    "import wbdata\n",
    "\n",
    "def date_format(in_date):\n",
    "    # converting date from textbased to dd-mm-yyyy format\n",
    "    if \"-\" in in_date:\n",
    "        split_date = in_date.split(\"-\")\n",
    "    else:\n",
    "        split_date = in_date.split(\" \")\n",
    "\n",
    "    month_num = month_convert(split_date[1])\n",
    "    if int(split_date[2]) < 50:\n",
    "        year = int(split_date[2]) + 2000\n",
    "    else:\n",
    "        year = int(split_date[2])\n",
    "    out_date = split_date[0] + \"-\" + str(month_num) + \"-\" + str(year)\n",
    "    return out_date\n",
    "\n",
    "def month_convert(month_name):\n",
    "    months = {\n",
    "    \"jan\": \"01\", \"january\": \"01\",\n",
    "    \"feb\": \"02\", \"february\": \"02\",\n",
    "    \"mar\": \"03\", \"march\": \"03\",\n",
    "    \"apr\": \"04\", \"april\": \"04\",\n",
    "    \"may\": \"05\", \"may\": \"05\",\n",
    "    \"jun\": \"06\", \"june\": \"06\",\n",
    "    \"jul\": \"07\", \"july\": \"07\",\n",
    "    \"aug\": \"08\", \"august\": \"08\",\n",
    "    \"sep\": \"09\", \"september\": \"09\",\n",
    "    \"oct\": \"10\", \"october\": \"10\",\n",
    "    \"nov\": \"11\", \"november\": \"11\",\n",
    "    \"dec\": \"12\", \"december\": \"12\"\n",
    "    }\n",
    "\n",
    "    # Convert the month name to lowercase and strip leading/trailing whitespace\n",
    "    month_name = month_name.strip().lower()\n",
    "\n",
    "    # Look up the month number in the dictionary\n",
    "    if month_name in months:\n",
    "        month_num = months[month_name]\n",
    "        #print(f\"The month number for {month_name} is {month_num}.\")\n",
    "    else:\n",
    "        print(\"Invalid month name entered.\")\n",
    "\n",
    "    return month_num\n",
    "\n",
    "def between_date(d1, d2):\n",
    "    # Gets difference between two dates in string format \"dd-mm-yyyy\"\n",
    "    d1list = d1.split(\"-\")\n",
    "    d2list = d2.split(\"-\")\n",
    "    date1 = datetime(int(d1list[2]), int(d1list[1]), int(d1list[0]))\n",
    "    date2 = datetime(int(d2list[2]), int(d2list[1]), int(d2list[0]))\n",
    "\n",
    "    return abs((date1 - date2).days)  # Maybe add +1\n",
    "\n",
    "def drop_rows(inputdata, columnname, dropparameter):\n",
    "    removedrows = inputdata.index[\n",
    "        inputdata[columnname] <= dropparameter].tolist()\n",
    "    outputdata = inputdata.drop(removedrows)\n",
    "    return outputdata\n",
    "\n",
    "def get_state_population(state_name, population_input_file):\n",
    "    df = pd.read_csv(population_input_file)\n",
    "    filtered_df = df[df['name'] == state_name]\n",
    "    if len(filtered_df) > 0:\n",
    "        population = filtered_df['population'].values[0]\n",
    "        return population\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_city_population(city_name,population_input_file):\n",
    "    country_code = 'ML'\n",
    "    url = \"https://wft-geo-db.p.rapidapi.com/v1/geo/cities/{0}\".format(get_wikidata_id(city_name))\n",
    "\n",
    "    headers = {\n",
    "        \"X-RapidAPI-Key\": \"6e1b07b54fmsh14df87e58d9db7bp175272jsn85fd0398365f\",\n",
    "        \"X-RapidAPI-Host\": \"wft-geo-db.p.rapidapi.com\"\n",
    "    }\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.status_code == 404:\n",
    "        get_state_population(city_name,population_input_file)\n",
    "\n",
    "    else:\n",
    "        data = response.json()\n",
    "        population = data[\"data\"]['population']\n",
    "        return population\n",
    "    \n",
    "def filter_table(df, colname, adminlevel):\n",
    "    if adminlevel == \"admin1\":\n",
    "        adminlist = df.admin1.unique()\n",
    "    elif adminlevel == \"location\":\n",
    "        adminlist = df.location.unique()\n",
    "    else:\n",
    "        adminlist = df.admin2.unique()\n",
    "\n",
    "    newdf = pd.DataFrame()  \n",
    "\n",
    "    for admin in adminlist:\n",
    "        tempdf = df.loc[df[adminlevel] == admin]\n",
    "        tempdf.sort_values(colname, ascending=True)\n",
    "        newdf = pd.concat([newdf, tempdf.tail(1)])\n",
    "\n",
    "    return newdf\n",
    "\n",
    "\n",
    "def acled2locations(country, start_date, filter_opt, location_type=\"admin1\", conflict_threshold=100):\n",
    "    current_dir = os.getcwd()\n",
    "    input_file = os.path.join(current_dir, country, \"acled.csv\") \n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except:\n",
    "        print(\"Runtime Error: File Cannot be found\")\n",
    "        return\n",
    "    \n",
    "    df = df[[\"event_date\", \"country\", \"admin1\", \"admin2\", \"location\", \"latitude\", \"longitude\", \"event_type\", \"sub_event_type\", \"fatalities\"]]\n",
    "    \n",
    "    event_dates = df[\"event_date\"].tolist()\n",
    "    \n",
    "    formatted_event_dates = [date_format(date) for date in event_dates]\n",
    "    \n",
    "    # Calculate the number of days between each event's date and the start_date\n",
    "    conflict_dates = [between_date(d, start_date) for d in formatted_event_dates]\n",
    "    \n",
    "    # Add days to the df\n",
    "    df['conflict_date'] = conflict_dates\n",
    "    \n",
    "    # Including all events\n",
    "    fatalities_threshold = 0\n",
    "    \n",
    "    # Dropping rows based on the 'fatalities_threshold'    \n",
    "    df = drop_rows(df, 'fatalities', fatalities_threshold)\n",
    "    \n",
    "    # Sorting the df by 'conflict_date' and 'admin1' and drops duplicate rows\n",
    "    df = df.sort_values([\"conflict_date\", \"admin1\"]).drop_duplicates([\"conflict_date\", \"admin1\"])\n",
    "    \n",
    "    if filter_opt == 'earliest':\n",
    "        filter_opt = 'conflict_date'\n",
    "\n",
    "    try:\n",
    "        df = filter_table(df, filter_opt, location_type)\n",
    "    except:\n",
    "        print(\"Runtime error: filter_opt value must be earliest or fatalities\")\n",
    "        \n",
    "    output_df = df[['admin1', 'country', 'latitude', 'longitude', 'conflict_date']]\n",
    "    \n",
    "    output_df.columns = ['name', 'country', 'latitude', 'longitude', 'conflict_date'] \n",
    "    \n",
    "    # Create two DataFrames for towns and conflict zones\n",
    "    towns_df = df[df['conflict_date'] <= conflict_threshold].copy()\n",
    "    conflict_zones_df = df[df['conflict_date'] > conflict_threshold].copy()\n",
    "    \n",
    "    # Assign location types\n",
    "    towns_df['location_type'] = 'town'\n",
    "    conflict_zones_df['location_type'] = 'conflict_zone'\n",
    "    \n",
    "    # Merge the DataFrames back together if necessary\n",
    "    merged_df = pd.concat([towns_df, conflict_zones_df])\n",
    "    \n",
    "    # Create a dictionary to store location names from population.csv as keys and their populations as values\n",
    "    population_dict = {row['name']: row['population'] for index, row in population_df.iterrows()}\n",
    "    \n",
    "    # Add population to the merged_df based on 'admin1' names\n",
    "    merged_df['population'] = [population_dict.get(name, 0) for name in merged_df['admin1']]\n",
    "    \n",
    "    # Replace any infinite or NaN values with 0 and convert population to integers\n",
    "    merged_df['population'] = merged_df['population'].replace([np.inf, -np.inf, np.nan], 0)\n",
    "    merged_df['population'] = merged_df['population'].astype(int)\n",
    "    \n",
    "    # Reorder columns and rename them\n",
    "    merged_df = merged_df[['admin1', 'country', 'latitude', 'longitude', 'location_type', 'conflict_date', 'population']]\n",
    "    merged_df.columns = ['name', 'country', 'latitude', 'longitude', 'location_type', 'conflict_date', 'population']\n",
    "\n",
    "    # Save the final 'locations.csv' file\n",
    "    output_file = os.path.join(country, \"locations.csv\")\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f'{country}/locations.csv created. Please inspect the file for unwanted anomalies!')\n",
    "\n",
    "    \n",
    "# Please replace the country name \n",
    "country = \"nigeria2016\"\n",
    "start_date = \"01-01-2016\"\n",
    "filter_opt = 'earliest'\n",
    "location_type = \"admin1\"\n",
    "conflict_threshold = 100\n",
    "\n",
    "population_input_file = os.path.join(country, \"population.csv\")\n",
    "\n",
    "acled2locations(country, start_date, filter_opt, location_type=\"admin1\", conflict_threshold=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2238a5c",
   "metadata": {},
   "source": [
    "### Create Conflicts File\n",
    "\n",
    "This builder constructs conflicts.csv from created locations.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f4da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "\n",
    "# Replace path with the actual directory path\n",
    "flee_path = '/home/mghorbani/workspace/flee'\n",
    "sys.path.append(flee_path)\n",
    "\n",
    "# Now you can import the modules from the 'flee' package\n",
    "from flee.InputGeography import InputGeography\n",
    "\n",
    "def find_column_index(header, column_name):\n",
    "    # Find the index of a column by matching its name in the header\n",
    "    for i, col in enumerate(header):\n",
    "        if col == column_name:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def location2conflict(simulation_period, input_file, output_file):\n",
    "    ig = InputGeography()\n",
    "    ig.ReadLocationsFromCSV(input_file)\n",
    "\n",
    "    with open(input_file, \"r\") as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)\n",
    "        conflict_zone_index = find_column_index(header, \"location_type\")\n",
    "        conflict_date_index = find_column_index(header, \"conflict_date\")\n",
    "\n",
    "    # Create the header string for the output file without the \"name\" column\n",
    "    output_header_string = \"day\"\n",
    "    for l in ig.locations:\n",
    "        if l[conflict_zone_index] == \"conflict_zone\":\n",
    "            output_header_string += \",%s\" % l[0]\n",
    "    output_header_string += \"\\n\"\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(output_header_string)\n",
    "\n",
    "        for t in range(0, simulation_period):\n",
    "            output = \"%s\" % t\n",
    "            for l in ig.locations:\n",
    "                if l[conflict_zone_index] == \"conflict_zone\":\n",
    "                    confl_date = int(l[conflict_date_index])\n",
    "                    if confl_date <= t:\n",
    "                        output += \",1\"\n",
    "                    else:\n",
    "                        output += \",0\"\n",
    "            output += \"\\n\"\n",
    "            file.write(output)\n",
    "\n",
    "\n",
    "# Set the values for simulation_period, input_file, and output_file\n",
    "simulation_period = 365\n",
    "\n",
    "# Please specify a country name\n",
    "country = \"nigeria2016\"\n",
    "\n",
    "input_file = os.path.join(country, \"locations.csv\")\n",
    "\n",
    "output_file = os.path.join(country, \"conflicts.csv\")\n",
    "\n",
    "# Call the function location2conflict\n",
    "location2conflict(simulation_period, input_file, output_file)\n",
    "\n",
    "print(f'{country}/conflicts.csv created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174bcfe1",
   "metadata": {},
   "source": [
    "###  Create Conflict Scenario\n",
    "\n",
    "This builder simulates a conflict scenario in a given country using custom distribution function. \n",
    "\n",
    "Change function parameters to create different distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a365ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "# Read \n",
    "def read_conflict_zones(filename):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        conflict_zones = df.columns[1:].tolist()\n",
    "        conflict_zones = [zone for zone in conflict_zones if zone]  # Exclude empty headers\n",
    "        return conflict_zones\n",
    "    except FileNotFoundError:\n",
    "        print(\"File not found: \", filename)\n",
    "        return []\n",
    "    \n",
    "    \n",
    "# Custom function to generate all zeros csv file\n",
    "def generate_conflict_zones_csv(filename, conflict_zones, period):\n",
    "    data = {'Days': list(range(period))}\n",
    "    data.update({zone: [0] * period for zone in conflict_zones})\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    \n",
    "\n",
    "# Custom distribution function\n",
    "def custom_distribution(intensity, x):\n",
    "    max_value = intensity\n",
    "    peak_day = 365\n",
    "    std_deviation = 800\n",
    "    variation_factor = 0.01  # Adjust the variation factor as desired\n",
    "    \n",
    "    spreading_factor = np.exp(-((x - peak_day) / std_deviation) ** 2)\n",
    "    \n",
    "    # Add random fluctuations to the spreading factor\n",
    "    spreading_factor += np.random.normal(0, variation_factor, len(x))\n",
    "    \n",
    "    y = max_value * spreading_factor\n",
    "    return y\n",
    "\n",
    "# Specify the simulation country\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Create the path to input file\n",
    "input_file = os.path.join(country, 'conflicts.csv')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Call the function to extract the conflict_zones from the file\n",
    "conflict_zones = read_conflict_zones(input_file)\n",
    "\n",
    "# Set the intensity as maximum number of conflict zones \n",
    "intensity = len(conflict_zones)\n",
    "\n",
    "# Generate x-axis values\n",
    "start_date = date(2016, 1, 1)\n",
    "current_date = date(2016, 12, 31)\n",
    "days_passed = (current_date - start_date).days\n",
    "\n",
    "# Specify the simulation period for forcasting\n",
    "period = days_passed * 2\n",
    "\n",
    "# Generate x-axis values using a custom_distributions\n",
    "x1 = np.linspace(0, days_passed, num=days_passed).astype(int)\n",
    "x2 = np.linspace(days_passed, period, num=period - days_passed).astype(int)\n",
    "\n",
    "# Generate y-axis values using conflicts values\n",
    "y1 = df.iloc[:, 1:days_passed + 1].sum(axis=1)[:days_passed]\n",
    "\n",
    "# Generate y-axis values using a custom_distribution\n",
    "y2 = custom_distribution(intensity, x2)\n",
    "\n",
    "# Combine the generated data\n",
    "x = np.concatenate((x1, x2))\n",
    "y = np.concatenate((y1, y2))\n",
    "\n",
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "# Plot graph\n",
    "ax1.plot(x, y)\n",
    "ax1.set_title('Custom Distribution')\n",
    "ax1.set_xlabel('Days')\n",
    "ax1.set_ylabel('Conflict Zones')\n",
    "\n",
    "# Convert y values to integers\n",
    "y = [int(val) for val in y]\n",
    "\n",
    "# Create path to modified CSV file\n",
    "output_file = os.path.join(country, \"simulated-conflicts.csv\")\n",
    "\n",
    "# Call the function to generate all zeros csv file\n",
    "generate_conflict_zones_csv(output_file, conflict_zones, period)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_modified = pd.read_csv(output_file)\n",
    "\n",
    "# Update the modified DataFrame with the new y values\n",
    "modified_rows = []\n",
    "y_index = 0\n",
    "for _, row in df_modified.iterrows():\n",
    "    row = row.values\n",
    "    if y_index >= len(y):\n",
    "        break\n",
    "\n",
    "    number = y[y_index]\n",
    "    y_index += 1\n",
    "    assigned_count = 0\n",
    "    for i in range(1, len(row)):\n",
    "        if assigned_count < number:\n",
    "            row[i] = 1\n",
    "            assigned_count += 1\n",
    "        else:\n",
    "            row[i] = 0\n",
    "\n",
    "    modified_rows.append(row)\n",
    "    \n",
    "# Create modified DataFrame\n",
    "modified_df = pd.DataFrame(modified_rows, columns=df.columns)\n",
    "\n",
    "# Compute the sum of each row (excluding the '#Day' column)\n",
    "sum_values = modified_df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Plot the summed values\n",
    "ax2.plot(x, sum_values)\n",
    "ax2.set_title('simulated-conflicts.csv')\n",
    "ax2.set_xlabel('Days')\n",
    "\n",
    "# Save the modified DataFrame to the output file\n",
    "modified_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'{country}/simulated-conflicts.csv created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e02fce",
   "metadata": {},
   "source": [
    "### Create Routes Between Locations & Distances (No Google Map API)\n",
    "\n",
    "This builder construct routes.csv from created locations.csv file.\n",
    "\n",
    "Before creating routes, add camps to the locations.csv, following the header format. \n",
    "\n",
    "Example: There is a sample file \"camps.csv\" in nigeria2016 directory. Copy its content to the end of locations.csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1b6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "import csv\n",
    "import webbrowser\n",
    "\n",
    "# Please replace country if required\n",
    "country = \"nigeria2016\"\n",
    "\n",
    "# Read the locations from the CSV file\n",
    "df = pd.read_csv(f'{country}/locations.csv')\n",
    "\n",
    "# Initialise a list to store route information\n",
    "routes = []\n",
    "\n",
    "# Calculate routes and distances between locations\n",
    "for i, loc1 in df.iterrows():\n",
    "    for j, loc2 in df.iterrows():\n",
    "        if i != j:\n",
    "            loc1_coords = (loc1['latitude'], loc1['longitude'])\n",
    "            loc2_coords = (loc2['latitude'], loc2['longitude'])\n",
    "            \n",
    "            # Calculate the distance between loc1_coords and loc2_coords\n",
    "            distance = geodesic(loc1_coords, loc2_coords).km\n",
    "            \n",
    "            # Add route information to the list\n",
    "            routes.append([loc1['name'], loc2['name'], distance, 0]) \n",
    "\n",
    "# Save the routes to a CSV file\n",
    "with open(f'{country}/routes.csv', 'w', newline='') as csvfile:\n",
    "    fieldnames = ['name1', 'name2', 'distance', 'foreced_redirection']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    \n",
    "    # Write the header\n",
    "    writer.writeheader()\n",
    "    \n",
    "    # Write the route information\n",
    "    for route in routes:\n",
    "        writer.writerow({'name1': route[0], 'name2': route[1], 'distance': round(route[2], 3), 'foreced_redirection': route[3]})\n",
    "\n",
    "# Add the map layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "print(f'{country}/routes.csv created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef01c5f",
   "metadata": {},
   "source": [
    "### Experimental Code "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d4f138",
   "metadata": {},
   "source": [
    "### Create & Store Route Coordinates in JSON Format (with Google Map API)\n",
    "\n",
    "This builder constructs routes and stores them in a JSON file using Google Map API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2b09125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigeria2016/routes.json created.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from geopy.distance import geodesic\n",
    "import polyline\n",
    "import json\n",
    "from multiprocessing import Pool\n",
    "import pandas as pd\n",
    "\n",
    "# Function to decode a polyline string into a list of locations\n",
    "def decode_polyline(polyline_str):\n",
    "    return polyline.decode(polyline_str)\n",
    "\n",
    "# Function to fetch routes for a pair of locations\n",
    "def fetch_route(loc1_name, loc2_name, loc1_coords, loc2_coords):\n",
    "    # Calculate the distance between loc1_coords and loc2_coords\n",
    "    distance = geodesic(loc1_coords, loc2_coords).km\n",
    "\n",
    "    # Limit the search to locations within a certain distance threshold\n",
    "    if distance <= 500:  # Higher value results in more routes\n",
    "        # Use Google Maps Directions API to obtain the route\n",
    "        api_url = \"https://maps.googleapis.com/maps/api/directions/json\"\n",
    "        params = {\n",
    "            \"origin\": f\"{loc1_coords[0]},{loc1_coords[1]}\",\n",
    "            \"destination\": f\"{loc2_coords[0]},{loc2_coords[1]}\",\n",
    "            \"key\": \"AIzaSyCFayFsbHfcA0GuOfhqaRqec3w90A9lbt0\"  # Replace with your own Google Maps API key\n",
    "        }\n",
    "        response = requests.get(api_url, params=params)\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the polyline representing the route\n",
    "        if data[\"status\"] == \"OK\":\n",
    "            polyline_points = data[\"routes\"][0][\"overview_polyline\"][\"points\"]\n",
    "            polyline_locations = decode_polyline(polyline_points)\n",
    "\n",
    "            # Create a route with the polyline locations\n",
    "            route = {\n",
    "                \"name1\": loc1_name,\n",
    "                \"name2\": loc2_name,\n",
    "                \"route\": polyline_locations\n",
    "            }\n",
    "            return route\n",
    "    return None\n",
    "\n",
    "# Please replace country if required\n",
    "country = 'nigeria2016'\n",
    "\n",
    "# Read the locations from the CSV file\n",
    "df = pd.read_csv('{}/locations.csv'.format(country))\n",
    "\n",
    "# Generate unique pairs of location coordinates and names to fetch routes for\n",
    "location_pairs = []\n",
    "for i, loc1_row in df.iterrows():\n",
    "    for j, loc2_row in df.iterrows():\n",
    "        if i != j:  # Ensure loc1 and loc2 are not the same\n",
    "            loc1_coords = (loc1_row['latitude'], loc1_row['longitude'])\n",
    "            loc2_coords = (loc2_row['latitude'], loc2_row['longitude'])\n",
    "            loc1_name = loc1_row['name']\n",
    "            loc2_name = loc2_row['name']\n",
    "            location_pairs.append((loc1_name, loc2_name, loc1_coords, loc2_coords))\n",
    "\n",
    "# Define the number of processes to use (adjust as needed)\n",
    "num_processes = multiprocessing.cpu_count()\n",
    "\n",
    "# Use multiprocessing to fetch routes\n",
    "with Pool(num_processes) as pool:\n",
    "    routes = pool.starmap(fetch_route, location_pairs)\n",
    "\n",
    "# Filter out None values (routes not found within the threshold)\n",
    "routes = [route for route in routes if route is not None]\n",
    "\n",
    "# Save routes as a dictionary to a JSON file\n",
    "with open('{}/routes.json'.format(country), 'w') as json_file:\n",
    "    json.dump(routes, json_file)\n",
    "\n",
    "print(f'{country}/routes.json created.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c064f648",
   "metadata": {},
   "source": [
    "### Filter Redundant Routes\n",
    "\n",
    "This builder calculates the Haversine distance between coordinates to split routes into subroutes. Then checks if one route contains another route with a given tolerance to filter routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66c8a2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigeria2016/non_redundant_routes.json created.\n",
      "Number of routes: 528\n",
      "Number of remaining routes: 195\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import multiprocessing\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "# Replace the country name, if required\n",
    "country = \"nigeria2016\"\n",
    "\n",
    "# Load the JSON file containing route data into a list of dictionaries\n",
    "with open(f'{country}/routes.json', 'r') as json_file:\n",
    "    routes_data = json.load(json_file)\n",
    "\n",
    "# Function to calculate the Haversine distance between two coordinates in kilometers\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Haversine formula\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = sin(dlat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(dlon / 2) ** 2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    distance = 6371 * c  # Radius of Earth in kilometers\n",
    "    return distance\n",
    "\n",
    "# Function to split a route into subroutes\n",
    "def split_route(route):\n",
    "    subroutes = []\n",
    "    current_subroute = []\n",
    "\n",
    "    for coord in route:\n",
    "        lat, lon = map(float, coord)  # Convert strings to floats\n",
    "        if current_subroute:\n",
    "            prev_lat, prev_lon = current_subroute[-1]\n",
    "            distance = haversine(prev_lat, prev_lon, lat, lon)\n",
    "            if distance > tolerance:\n",
    "                subroutes.append(current_subroute)\n",
    "                current_subroute = []\n",
    "        \n",
    "        current_subroute.append((lat, lon))\n",
    "\n",
    "    if current_subroute:\n",
    "        subroutes.append(current_subroute)\n",
    "\n",
    "    return subroutes\n",
    "\n",
    "# Function to check if route1 contains route2 with tolerance\n",
    "def route_contains_route(route1, route2, tolerance):\n",
    "    subroutes1 = split_route(route1)\n",
    "    subroutes2 = split_route(route2)\n",
    "\n",
    "    for subroute2 in subroutes2:\n",
    "        contains_subroute = False\n",
    "        for subroute1 in subroutes1:\n",
    "            if all(\n",
    "                haversine(coord1[0], coord1[1], coord2[0], coord2[1]) <= tolerance\n",
    "                for coord1, coord2 in zip(subroute1, subroute2)\n",
    "            ):\n",
    "                contains_subroute = True\n",
    "                break\n",
    "\n",
    "        if not contains_subroute:\n",
    "            return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Increase tolerance to filter more routes\n",
    "tolerance = 100.0 # kilometers\n",
    "\n",
    "def process_route(i):\n",
    "    route1 = routes_data[i]\n",
    "    redundant = False\n",
    "\n",
    "    # Compare route1 with other routes\n",
    "    for j, route2 in enumerate(routes_data):\n",
    "        if i != j:  # Avoid self-comparison\n",
    "            if route_contains_route(route1['route'], route2['route'], tolerance):\n",
    "                redundant = True\n",
    "                break\n",
    "\n",
    "    return not redundant, route1\n",
    "\n",
    "num_cpus = multiprocessing.cpu_count()\n",
    "    \n",
    "# Create a pool of worker processes\n",
    "with multiprocessing.Pool(processes=num_cpus) as pool:\n",
    "    results = pool.map(process_route, range(len(routes_data)))\n",
    "\n",
    "non_redundant_routes = [route for is_non_redundant, route in results if is_non_redundant]\n",
    "\n",
    "# Write the non-redundant routes back to the JSON file\n",
    "with open(f'{country}/non_redundant_routes.json', 'w') as output_file:\n",
    "    json.dump(non_redundant_routes, output_file)\n",
    "\n",
    "print(f'{country}/non_redundant_routes.json created.')\n",
    "\n",
    "# Print the number of routes\n",
    "num_routes = len(routes_data)\n",
    "print(f\"Number of routes: {num_routes}\")\n",
    "\n",
    "# Print the number of non-redundant routes\n",
    "num_remaining_routes = len(non_redundant_routes)\n",
    "print(f\"Number of remaining routes: {num_remaining_routes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2e7ca3",
   "metadata": {},
   "source": [
    "### Visualize Locations and Routes\n",
    "\n",
    "This builder reads locations and routes to display them on a map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "db798644",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Load locations data\n",
    "country = \"nigeria2016\"  # Replace with your country\n",
    "\n",
    "locations_df = pd.read_csv(f\"{country}/locations.csv\")\n",
    "\n",
    "# Create a map centered around the mean latitude and longitude\n",
    "map_center = [locations_df[\"latitude\"].mean(), locations_df[\"longitude\"].mean()]\n",
    "m = folium.Map(location=map_center, zoom_start=6)\n",
    "\n",
    "# Create location markers for conflict zones, towns, and camps\n",
    "for index, row in locations_df.iterrows():\n",
    "    location = [row[\"latitude\"], row[\"longitude\"]]\n",
    "    popup = f\"{row['name']} - {row['location_type']}\"\n",
    "    color = \"red\" if row[\"location_type\"] == \"conflict_zone\" else \"orange\" if row[\"location_type\"] == \"town\" else \"green\"\n",
    "    folium.CircleMarker(\n",
    "        location=location,\n",
    "        radius=6,\n",
    "        color=color,\n",
    "        fill=True,\n",
    "        fill_color=color,\n",
    "        popup=popup\n",
    "    ).add_to(m)\n",
    "\n",
    "# Load filtered routes data\n",
    "with open(f\"{country}/non_redundant_routes.json\", \"r\") as json_file:\n",
    "    filtered_routes = json.load(json_file)\n",
    "\n",
    "# Add filtered routes to the map\n",
    "for route_data in filtered_routes:\n",
    "    route = route_data[\"route\"]\n",
    "    route_coordinates = [(coord[0], coord[1]) for coord in route]\n",
    "    color = \"blue\"  # You can choose a different color for routes\n",
    "    folium.PolyLine(\n",
    "        locations=route_coordinates,\n",
    "        color=color,\n",
    "        weight=2.5\n",
    "    ).add_to(m)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "m.save(f\"{country}/map.html\")\n",
    "\n",
    "# Open the map in a web browser\n",
    "webbrowser.open(f\"{country}/map.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8def38",
   "metadata": {},
   "source": [
    "### Create Filtered Routes \n",
    "\n",
    "This builder creates a new routes.csv with only non-redundant routes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8ba52149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigeria2016/new_routes.csv created.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Replace with your country and file paths\n",
    "country = \"nigeria2016\"\n",
    "\n",
    "routes_csv_path = f'{country}/routes.csv'\n",
    "non_redundant_routes_json_path = f'{country}/non_redundant_routes.json'\n",
    "new_routes_csv_path = f'{country}/new_routes.csv'\n",
    "\n",
    "# Load non-redundant routes from the JSON file\n",
    "with open(non_redundant_routes_json_path, 'r') as json_file:\n",
    "    non_redundant_routes = json.load(json_file)\n",
    "\n",
    "# Create a dictionary to store distances between locations\n",
    "distances = {}\n",
    "\n",
    "# Read existing distances from routes.csv\n",
    "with open(routes_csv_path, 'r') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        name1 = row['name1']\n",
    "        name2 = row['name2']\n",
    "        distance = float(row['distance'])\n",
    "        distances[(name1, name2)] = distance\n",
    "\n",
    "# Filter distances based on non-redundant routes\n",
    "filtered_distances = {}\n",
    "for route in non_redundant_routes:\n",
    "    name1 = route['name1']\n",
    "    name2 = route['name2']\n",
    "    if (name1, name2) in distances:\n",
    "        filtered_distances[(name1, name2)] = distances[(name1, name2)]\n",
    "\n",
    "# Write the filtered distances to the new routes.csv file\n",
    "with open(new_routes_csv_path, 'w', newline='') as csv_file:\n",
    "    fieldnames = ['name1', 'name2', 'distance', 'forced_redirection']\n",
    "    csv_writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "    csv_writer.writeheader()\n",
    "\n",
    "    for (name1, name2), distance in filtered_distances.items():\n",
    "        csv_writer.writerow({'name1': name1, 'name2': name2, 'distance': distance, 'forced_redirection': 0})\n",
    "\n",
    "print(f'{country}/new_routes.csv created.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
